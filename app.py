import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import plotly.express as px
from datetime import datetime, timedelta
import re

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼ˆé¿å…è­¦å‘Šï¼‰
plt.rcParams['font.family'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Trend Analysis Dashboard",
    page_icon="ğŸ“Š",
    layout="wide"
)

st.title("ğŸ“Š Real-time Trend Analysis Dashboard")
st.markdown("---")

# åˆå§‹åŒ–session state
if 'last_refresh' not in st.session_state:
    st.session_state.last_refresh = None
if 'data_cache' not in st.session_state:
    st.session_state.data_cache = None

def get_english_stopwords():
    """è·å–è‹±æ–‡åœç”¨è¯åˆ—è¡¨"""
    english_stopwords = {
        # åŸºç¡€å† è¯ã€è¿è¯ã€ä»‹è¯
        'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'until', 'while', 
        'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 
        'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 
        'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 
        'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 
        'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 
        'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', "don't", "should", 
        "now", "'s", "'t", "'m", "'re", "'ve", "'d", "'ll", "n't", 'be', 'is', 'are', 
        'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 
        'did', 'doing', 
        
        # æ–°å¢è¿è¯
        'also', 'although', 'though', 'since', 'unless', 'whether', 'while', 'whereas',
        'therefore', 'thus', 'hence', 'consequently', 'moreover', 'furthermore', 
        'however', 'nevertheless', 'nonetheless', 'otherwise', 'instead', 'meanwhile',
        
        # æ—¶é—´ç›¸å…³è¯æ±‡
        'year', 'years', 'month', 'months', 'week', 'weeks', 'day', 'days', 'hour', 
        'hours', 'minute', 'minutes', 'second', 'seconds', 'time', 'times', 'season',
        'seasons', 'today', 'tomorrow',
        'yesterday', 'now', 'then', 'when', 'before', 'after', 'during', 'while',
        'moment', 'period', 'date', 'calendar', 'clock', 'schedule',
        
        # æ•°å­—å’Œåºæ•°è¯
        'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',
        'first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth',
        'ninth', 'tenth', 'once', 'twice', 'thrice', 'single', 'double', 'triple',
        'number', 'numbers', 'count', 'total', 'amount', 'quantity',
        
        # å¸¸è§ä»£è¯å’Œäººç§°
        'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
        'my', 'your', 'his', 'its', 'our', 'their', 'mine', 'yours', 'hers', 'ours',
        'theirs', 'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves',
        'yourselves', 'themselves',
        
        # å¸¸è§åŠ©åŠ¨è¯å’Œæƒ…æ€åŠ¨è¯
        'may', 'might', 'must', 'shall', 'should', 'would', 'could', 'ought','said'
        
        # æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
        '', ' ', '  ', ',', '.', '!', '?', ':', ';', '-', '(', ')', 
        '[', ']', '{', '}', '/', '\\', '|', '@', '#', '$', '%', '^', '&', '*', '+', '=', 
        '<', '>', '~', '_', '"', "'", '`',
        
        # å¸¸è§æ— æ„ä¹‰è¯æ±‡
        'very', 'really', 'quite', 'rather', 'pretty', 'just', 'even', 'still', 'yet',
        'already', 'almost', 'nearly', 'hardly', 'scarcely', 'simply', 'merely',
        'actually', 'basically', 'essentially', 'literally', 'virtually'
    }
    
    # æ·»åŠ å•ä¸ªå­—æ¯
    english_stopwords.update([chr(i) for i in range(97, 123)])
    english_stopwords.update([chr(i) for i in range(65, 91)])
    
    # æ·»åŠ æ•°å­—
    english_stopwords.update([str(i) for i in range(0, 100)])
    
    return english_stopwords

def is_english_word(word):
    """æ£€æŸ¥å•è¯æ˜¯å¦åªåŒ…å«è‹±æ–‡å­—æ¯"""
    if not isinstance(word, str):
        return False
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥æ˜¯å¦åªåŒ…å«è‹±æ–‡å­—æ¯ï¼ˆå…è®¸è¿å­—ç¬¦å’Œæ’‡å·ï¼‰
    return bool(re.match(r'^[a-zA-Z\-\.\']+$', word))

def clean_with_stopwords(df, word_col='word'):
    """ä½¿ç”¨åœç”¨è¯åˆ—è¡¨æ¸…ç†æ•°æ®"""
    stop_words = get_english_stopwords()
    
    def is_valid_word(word):
        if pd.isna(word) or not isinstance(word, str):
            return False
        
        word_clean = word.strip().lower()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åœç”¨è¯
        if word_clean in stop_words:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åªåŒ…å«è‹±æ–‡å­—æ¯
        if not is_english_word(word):
            return False
        
        # æ£€æŸ¥å•è¯é•¿åº¦
        if len(word_clean) <= 1:
            return False
        
        # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯ç‰¹æ®Šå­—ç¬¦
        if re.match(r'^[^\w\s]+$', word_clean):
            return False
            
        return True
    
    original_count = len(df)
    cleaned_df = df[df[word_col].apply(is_valid_word)].copy()
    return cleaned_df, original_count - len(cleaned_df)

def generate_wordcloud(df, word_col='word', weight_col='frequency', title="Word Cloud", max_words=100):
    """ç”Ÿæˆè¯äº‘å›¾"""
    if df is None or len(df) == 0:
        return None
    try:
        word_freq = dict(zip(df[word_col], df[weight_col]))
        wordcloud = WordCloud(
            width=900, height=450, background_color='white',
            colormap='viridis', max_words=max_words, relative_scaling=0.5
        ).generate_from_frequencies(word_freq)
        
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title(title, fontsize=16, pad=20)
        return fig
    except Exception as e:
        st.error(f"Error generating wordcloud: {e}")
        return None

def generate_combined_wordcloud(df, word_col='word', freq_col='frequency', tfidf_col='score', title="Combined Word Cloud", max_words=100):
    """ç”Ÿæˆç»“åˆfrequencyå’ŒTF-IDFçš„è¯äº‘"""
    if df is None or len(df) == 0:
        return None
    try:
        # æ ‡å‡†åŒ–frequencyå’ŒTF-IDFåˆ†æ•°
        freq_values = df[freq_col].values
        tfidf_values = df[tfidf_col].values
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆfrequency * TF-IDFï¼‰
        combined_scores = freq_values * tfidf_values
        
        # åˆ›å»ºç»¼åˆæƒé‡å­—å…¸
        word_weights = dict(zip(df[word_col], combined_scores))
        
        wordcloud = WordCloud(
            width=900, height=450, background_color='white',
            colormap='viridis', max_words=max_words, relative_scaling=0.5
        ).generate_from_frequencies(word_weights)
        
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title(title, fontsize=16, pad=20)
        return fig, word_weights
    except Exception as e:
        st.error(f"Error generating combined wordcloud: {e}")
        return None, None



def load_data_files():
    """åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
    data_files = {}
    file_mappings = {
    'word_freq': 'word_frequency.csv',
    'word_freq_title': 'word_frequency_title.csv',
    'tfidf': 'tfidf.csv',
    'tfidf_title': 'tfidf_title.csv',
    'articles': 'articles.csv'
}

    
    for key, filename in file_mappings.items():
        try:
            if key == 'articles':
                df = pd.read_csv(filename)
                # å°è¯•è§£ææ—¶é—´åˆ—
                if 'published_time' in df.columns:
                    df['published_time'] = pd.to_datetime(df['published_time'], errors='coerce')
            else:
                df = pd.read_csv(filename)
            
            # ç»Ÿä¸€åˆ—å
            if key in ['word_freq', 'word_freq_title'] and 'count' in df.columns:
                df = df.rename(columns={'count': 'frequency'})
            
            data_files[key] = df
            print(f"âœ… Loaded {filename}")
            
        except FileNotFoundError:
            print(f"âŒ {filename} not found, using sample data")
            # åˆ›å»ºç¤ºä¾‹æ•°æ®
            if key == 'articles':
                data_files[key] = pd.DataFrame({
                    'id': [1, 2, 3], 'country': ['US', 'UK', 'CA'],
                    'platform': ['news.com', 'blog.org', 'forum.net'],
                    'published_time': pd.to_datetime(['2025-11-13 10:00:00', '2025-11-13 11:00:00', '2025-11-13 12:00:00']),
                    'title': ['Sample 1', 'Sample 2', 'Sample 3'],
                    'content': ['Content 1', 'Content 2', 'Content 3'],
                    'url': ['http://example.com/1', 'http://example.com/2', 'http://example.com/3']
                })
            else:
                data_files[key] = pd.DataFrame({
                    'word': ['technology', 'innovation', 'data', 'analysis', 'research'],
                    'frequency' if key in ['word_freq', 'word_freq_title'] else 'score': [100, 80, 60, 40, 20]
                })
    
    return data_files

import time
from datetime import datetime, timedelta

def update_data_cache():
    """æ›´æ–°æ•°æ®ç¼“å­˜"""
    with st.spinner("Loading data..."):
        data_files = load_data_files()
        
        # æ¸…ç†è¯æ±‡æ•°æ®
        for key in ['word_freq', 'word_freq_title', 'tfidf', 'tfidf_title']:
            if key in data_files:
                data_files[key], removed_count = clean_with_stopwords(data_files[key])
                print(f"Cleaned {key}: removed {removed_count} stopwords")
        
        # åˆ›å»ºåˆ†ææ•°æ®è¯å…¸
        analysis_data = {
            'word_data': {
                'content_freq': data_files.get('word_freq'),
                'content_tfidf': data_files.get('tfidf'),
                'title_freq': data_files.get('word_freq_title'),
                'title_tfidf': data_files.get('tfidf_title')
            },
            'platform_data': data_files.get('articles'),
            'last_update': datetime.now(),
            'top_words': {},
            'top_platforms': None
        }
        
        # é¢„è®¡ç®—topè¯æ±‡
        for data_type, df in analysis_data['word_data'].items():
            if df is not None and len(df) > 0:
                weight_col = 'frequency' if 'freq' in data_type else 'score'
                analysis_data['top_words'][data_type] = df.nlargest(20, weight_col)
        
        # é¢„è®¡ç®—topå¹³å°
        if analysis_data['platform_data'] is not None:
            platform_counts = analysis_data['platform_data']['platform'].value_counts().reset_index()
            platform_counts.columns = ['platform', 'count']
            analysis_data['top_platforms'] = platform_counts
        
        st.session_state.analysis_data = analysis_data
        st.session_state.last_refresh = datetime.now().strftime("%H:%M:%S")

def get_articles_by_platform_and_words(platforms, top_words, articles_df, max_platforms=15):
    """æ ¹æ®å¹³å°å’Œå…³é”®è¯è·å–ç›¸å…³æ–‡ç« """
    result = {}
    
    # è·å–topå¹³å°
    top_platform_list = platforms.head(max_platforms)['platform'].tolist()
    
    # è·å–topè¯æ±‡
    keyword_list = top_words['word'].tolist()
    
    for platform in top_platform_list:
        platform_articles = articles_df[articles_df['platform'] == platform]
        
        relevant_articles = []
        for _, article in platform_articles.iterrows():
            title = str(article.get('title', ''))
            url = article.get('url', '')
            
            # å°†æ ‡é¢˜åˆ†å‰²æˆå•è¯åˆ—è¡¨ï¼ˆåªåŒ¹é…å®Œæ•´å•è¯ï¼‰
            title_words = re.findall(r'\b\w+\b', title.lower())
            
            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å®Œæ•´çš„topè¯æ±‡
            for keyword in keyword_list:
                keyword_lower = keyword.lower()
                if keyword_lower in title_words:
                    relevant_articles.append({
                        'title': title,
                        'url': url,
                        'matched_keyword': keyword
                    })
                    break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±åœæ­¢ï¼Œé¿å…é‡å¤
        
        if relevant_articles:
            result[platform] = relevant_articles
    
    return result

def main():
    # åˆå§‹åŒ–session state
    if 'analysis_data' not in st.session_state:
        st.session_state.analysis_data = None
    if 'last_refresh' not in st.session_state:
        st.session_state.last_refresh = None
    if 'cache_expiry' not in st.session_state:
        st.session_state.cache_expiry = datetime.now()
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("Control Panel")
        
        data_type = st.radio("Data Type", ["Content Analysis", "Title Analysis"])
        weight_method = st.radio("Weight Method", ["Frequency", "TF-IDF", "Combined"])
        
        # ç¼“å­˜çŠ¶æ€æ˜¾ç¤º
        if st.session_state.analysis_data:
            last_update = st.session_state.analysis_data['last_update']
            st.write(f"Last update: {last_update.strftime('%H:%M:%S')}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç¼“å­˜ï¼ˆæ¯3å°æ—¶ï¼‰
            if datetime.now() > st.session_state.cache_expiry:
                st.warning("Cache expired, refreshing data...")
                update_data_cache()
                st.session_state.cache_expiry = datetime.now() + timedelta(hours=3)
        
        if st.button("ğŸ”„ Refresh Data Now"):
            update_data_cache()
            st.session_state.cache_expiry = datetime.now() + timedelta(hours=3)
            st.rerun()
    
    # åŠ è½½æˆ–æ›´æ–°æ•°æ®
    if st.session_state.analysis_data is None:
        update_data_cache()
        st.session_state.cache_expiry = datetime.now() + timedelta(hours=3)
    
    data = st.session_state.analysis_data
    
    # ç¡®å®šå½“å‰æ•°æ®æº
    if data_type == "Content Analysis":
        current_freq_data = data['word_data']['content_freq']
        current_tfidf_data = data['word_data']['content_tfidf']
        current_data = current_freq_data if weight_method == "Frequency" else current_tfidf_data
        top_words_key = 'content_freq' if weight_method == "Frequency" else 'content_tfidf'
    else:
        current_freq_data = data['word_data']['title_freq']
        current_tfidf_data = data['word_data']['title_tfidf']
        current_data = current_freq_data if weight_method == "Frequency" else current_tfidf_data
        top_words_key = 'title_freq' if weight_method == "Frequency" else 'title_tfidf'
    
    weight_col = 'frequency' if weight_method == "Frequency" else 'score'
    
    # ä¸»å†…å®¹åŒº - å¢åŠ ç¬¬å››ä¸ªæ ‡ç­¾é¡µç”¨äºæ–‡ç« è·³è½¬
    tab1, tab2, tab3, tab4 = st.tabs(["â˜ï¸ Word Cloud", "ğŸ“Š Platform Analysis", "ğŸ“ˆ Data Details", "ğŸ”— Article Links"])
    
    with tab1:
        if weight_method == "Combined":
            st.header(f"{data_type} - Combined Frequency & TF-IDF Word Cloud")
            if current_freq_data is not None and current_tfidf_data is not None:
                merged_data = pd.merge(current_freq_data, current_tfidf_data, on='word', suffixes=('_freq', '_tfidf'))
                combined_fig, combined_weights = generate_combined_wordcloud(
                    merged_data, 
                    freq_col='frequency', 
                    tfidf_col='score',
                    title=f"Combined Word Cloud ({data_type})"
                )
                if combined_fig:
                    st.pyplot(combined_fig)
                
                st.subheader("Top 10 Words (Combined Score)")
                if combined_weights:
                    top_words = sorted(combined_weights.items(), key=lambda x: x[1], reverse=True)[:10]
                    top_df = pd.DataFrame(top_words, columns=['word', 'combined_score'])
                    st.dataframe(top_df, use_container_width=True)
                    
                    # ä¿å­˜topè¯æ±‡ç”¨äºæ–‡ç« è·³è½¬
                    st.session_state.current_top_words = top_df
            else:
                st.warning("No data available for combined word cloud")
        
        else:
            st.header(f"{data_type} - {weight_method} Word Cloud")
            if current_data is not None and len(current_data) > 0:
                wordcloud_fig = generate_wordcloud(current_data, weight_col=weight_col)
                if wordcloud_fig:
                    st.pyplot(wordcloud_fig)
                
                st.subheader("Top 10 Words")
                top_data = current_data.nlargest(10, weight_col)
                st.dataframe(top_data, use_container_width=True)
                
                # ä¿å­˜topè¯æ±‡ç”¨äºæ–‡ç« è·³è½¬
                st.session_state.current_top_words = top_data
            else:
                st.warning("No data available")
    
    with tab2:
        st.header("Platform Distribution")
        if data['platform_data'] is not None and len(data['platform_data']) > 0:
            platform_counts = data['top_platforms']
            
            max_platforms = st.slider("Number of platforms to display", 
                                     min_value=5, 
                                     max_value=min(30, len(platform_counts)), 
                                     value=15)
            
            top_platforms = platform_counts.head(max_platforms)
            
            col1, col2 = st.columns(2)
            with col1:
                fig_pie = px.pie(
                    top_platforms, 
                    names='platform', 
                    values='count', 
                    title=f"Top {max_platforms} Platforms Distribution"
                )
                st.plotly_chart(fig_pie, use_container_width=True)
            
            with col2:
                fig_bar = px.bar(
                    top_platforms, 
                    x='count', 
                    y='platform', 
                    orientation='h', 
                    title=f"Top {max_platforms} Platforms",
                    color='count',
                    color_continuous_scale='viridis'
                )
                fig_bar.update_layout(
                    yaxis={'categoryorder': 'total ascending'},
                    showlegend=False
                )
                st.plotly_chart(fig_bar, use_container_width=True)
            
            # ä¿å­˜topå¹³å°ç”¨äºæ–‡ç« è·³è½¬
            st.session_state.current_top_platforms = top_platforms
            
            st.subheader("Platform Statistics")
            col_stat1, col_stat2, col_stat3 = st.columns(3)
            with col_stat1:
                st.metric("Total Platforms", len(platform_counts))
            with col_stat2:
                coverage = (top_platforms['count'].sum() / platform_counts['count'].sum() * 100) if platform_counts['count'].sum() > 0 else 0
                st.metric(f"Top {max_platforms} Platforms Coverage", f"{coverage:.1f}%")
            with col_stat3:
                st.metric("Most Frequent Platform", 
                         top_platforms.iloc[0]['platform'] if len(top_platforms) > 0 else "N/A")
        else:
            st.info("No article data available")
    
    with tab3:
        st.header("Data Statistics")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Last Update", st.session_state.last_refresh)
        with col2:
            if weight_method == "Combined":
                data_to_count = current_freq_data if current_freq_data is not None else current_data
            else:
                data_to_count = current_data
            st.metric("Total Words", len(data_to_count) if data_to_count is not None else 0)
        with col3:
            st.metric("Data Type", data_type)
        with col4:
            st.metric("Weight Method", weight_method)
    
    with tab4:
        st.header("ğŸ”— Relevant Articles by Platform")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ•°æ®
        if ('current_top_words' not in st.session_state or 
            'current_top_platforms' not in st.session_state or 
            data['platform_data'] is None):
            st.info("Please view Word Cloud and Platform Analysis first to load the necessary data.")
        else:
            top_words = st.session_state.current_top_words
            top_platforms = st.session_state.current_top_platforms
            
            st.subheader("Search Criteria")
            col1, col2 = st.columns(2)
            with col1:
                st.write("**Top Words:**", ", ".join(top_words['word'].head(10).tolist()))
            with col2:
                st.write("**Top Platforms:**", ", ".join(top_platforms['platform'].head(10).tolist()))
            
            if st.button("ğŸ” Find Relevant Articles"):
                with st.spinner("Searching for relevant articles..."):
                    relevant_articles = get_articles_by_platform_and_words(
                        top_platforms, 
                        top_words,
                        data['platform_data']
                    )
                    
                    if relevant_articles:
                        st.success(f"Found relevant articles from {len(relevant_articles)} platforms")
                        
                        for platform, articles in relevant_articles.items():
                            with st.expander(f"ğŸ“° {platform} ({len(articles)} articles)"):
                                for i, article in enumerate(articles, 1):
                                    st.write(f"{i}. **{article['title']}**")
                                    st.write(f"   ğŸ”— [Open Article]({article['url']})")
                                    st.write(f"   ğŸ¯ Matched keyword: `{article['matched_keyword']}`")
                                    st.write("---")
                    else:
                        st.warning("No relevant articles found matching the criteria.")

if __name__ == "__main__":
    main()
