import unittest
import pandas as pd
import numpy as np
from collections import deque
from datetime import datetime
from frequency_utils import calculate_word_trends, detect_burst_words
from history_queue import HistoryDataQueue
import numpy as np

# å‡è®¾ä½ çš„å‡½æ•°åœ¨ä¸€ä¸ªåä¸º frequency_utils çš„æ–‡ä»¶ä¸­
from frequency_utils import calculate_word_trends, detect_burst_words


# ========================================================
# æµ‹è¯•è¾…åŠ©ç±»ï¼šæ¨¡æ‹Ÿ HistoryDataQueue
# ========================================================
class MockHistoryQueue:
    """æ¨¡æ‹Ÿä½ çš„ HistoryDataQueue ç±»ï¼Œåªå®ç° get_all ç”¨äºæµ‹è¯•"""
    def __init__(self, data_list):
        # data_list æ ¼å¼: [(timestamp, df), (timestamp, df), ...]
        self.data = data_list

    def get_all(self):
        return self.data

# ========================================================
# å•å…ƒæµ‹è¯•ç±»
# ========================================================
class TestTrendAnalysis(unittest.TestCase):

    def setUp(self):
        # print("\n--- Setting up test data ---")
        pass

    # ------------------------------------------------
    # æµ‹è¯• calculate_word_trends
    # ------------------------------------------------
    def test_calculate_word_trends(self):
        print("\nğŸ§ª Testing calculate_word_trends...")

        # 1. æ„é€ æ—§æ•°æ® (Round 1)
        old_data = pd.DataFrame({
            'word': ['apple', 'banana', 'common_word'],
            'count': [10, 5, 20]
        })

        # 2. æ„é€ æ–°æ•°æ® (Round 2)
        # - apple: æ¶ˆå¤± (Lost)
        # - banana: ä¾ç„¶å­˜åœ¨ (Common)
        # - common_word: é¢‘ç‡å˜å¤§ (Common)
        # - durian: æ–°å‡ºç° (New)
        new_data = pd.DataFrame({
            'word': ['banana', 'common_word', 'durian'],
            'count': [5, 50, 15]
        })

        # 3. è¿è¡Œå‡½æ•°
        result = calculate_word_trends(old_data, new_data)

        # 4. éªŒè¯ 'new' (New words)
        new_df = result['new']
        print(f"  [New Words]: found {len(new_df)}")
        self.assertEqual(len(new_df), 1)
        self.assertEqual(new_df.iloc[0]['word'], 'durian')
        self.assertEqual(new_df.iloc[0]['trend'], 'new')

        # 5. éªŒè¯ 'lost' (Lost words)
        lost_df = result['lost']
        print(f"  [Lost Words]: found {len(lost_df)}")
        self.assertEqual(len(lost_df), 1)
        self.assertEqual(lost_df.iloc[0]['word'], 'apple')
        self.assertEqual(lost_df.iloc[0]['trend'], 'lost')

        # 6. éªŒè¯ 'common' (Common words)
        common_df = result['common']
        print(f"  [Common Words]: found {len(common_df)}")
        self.assertEqual(len(common_df), 2) # banana, common_word
        
        # éªŒè¯ growth rate calculation
        # common_word: old=20, new=50 -> change=30 -> rate=1.5
        word_row = common_df[common_df['word'] == 'common_word'].iloc[0]
        self.assertAlmostEqual(word_row['freq_change_rate'], 1.5)
        print("  âœ… calculate_word_trends Passed!")

    # ------------------------------------------------
    # æµ‹è¯• detect_burst_words
    # ------------------------------------------------
    def test_detect_burst_words(self):
        print("\nğŸ§ª Testing detect_burst_words...")

        # é…ç½®çª—å£å‚æ•°
        BASELINE_LEN = 8
        CURRENT_LEN = 2
        TOTAL_LEN = BASELINE_LEN + CURRENT_LEN

        # æ„é€ è™šæ‹Ÿæ•°æ®æµ
        # æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªè¯ "SuperTopic"ï¼Œåœ¨åŸºçº¿æœŸå¾ˆä½ï¼Œåœ¨å½“å‰çª—å£çªç„¶çˆ†å‘
        # æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªè¯ "NormalTopic"ï¼Œä¸€ç›´å¾ˆå¹³ç¨³
        
        history_data = []

        # --- Phase 1: Baseline (å‰8æ¬¡æ›´æ–°) ---
        for i in range(BASELINE_LEN):
            df = pd.DataFrame({
                'word': ['SuperTopic', 'NormalTopic', 'Noise'],
                'count': [1, 10, 1] # SuperTopic å¹³å‡ä¸º 1
            })
            history_data.append((datetime.now(), df))

        # --- Phase 2: Current (å2æ¬¡æ›´æ–°) ---
        for i in range(CURRENT_LEN):
            df = pd.DataFrame({
                'word': ['SuperTopic', 'NormalTopic', 'Noise'],
                'count': [50, 10, 1] # SuperTopic çªç„¶å˜æˆ 50
            })
            history_data.append((datetime.now(), df))

        # åˆ›å»º Mock é˜Ÿåˆ—
        mock_queue = MockHistoryQueue(history_data)

        # è¿è¡Œå‡½æ•°
        burst_df = detect_burst_words(
            mock_queue, 
            current_window_size=CURRENT_LEN, 
            baseline_window_size=BASELINE_LEN,
            min_freq_now=5,  # è¿‡æ»¤æ‰ Noise
            min_freq_base=0.5
        )

        print("  [Burst Result Table]:")
        print(burst_df.to_string())

        # éªŒè¯é€»è¾‘
        # 1. NormalTopic ä¸åº”è¯¥å‡ºç°åœ¨ç»“æœé‡Œï¼Œæˆ–è€… fold_change æ¥è¿‘ 1
        # å¦‚æœå®ƒæ²¡å‡ºç°ï¼ˆå› ä¸º min_freq_base æˆ–æ’åºï¼‰ï¼Œé‚£ä¹Ÿæ²¡äº‹ã€‚ä½†å¦‚æœå‡ºç°äº†ï¼Œfold_change åº”è¯¥æ˜¯ 1.0
        if 'NormalTopic' in burst_df['word'].values:
            row = burst_df[burst_df['word'] == 'NormalTopic'].iloc[0]
            self.assertAlmostEqual(row['fold_change'], 1.0, delta=0.2)

        # 2. SuperTopic åº”è¯¥æ˜¯çªå‘è¯
        # è®¡ç®—é¢„æœŸå€¼ï¼š
        # Baseline Total (8 frames) = 1 * 8 = 8. Window Ratio = 8/2 = 4.
        # Baseline Normalized = 8 / 4 = 2.0
        # Current Total (2 frames) = 50 + 50 = 100.
        # Fold Change = 100 / 2.0 = 50.0 (approx)
        
        target_row = burst_df[burst_df['word'] == 'SuperTopic']
        self.assertFalse(target_row.empty, "SuperTopic missed!")
        
        fold_change = target_row.iloc[0]['fold_change']
        freq_base = target_row.iloc[0]['freq_base']
        
        print(f"  [Verification] SuperTopic FoldChange: {fold_change} (Expected ~50.0)")
        
        self.assertAlmostEqual(freq_base, 2.0, delta=0.1)
        self.assertTrue(fold_change > 40, "Fold change should be huge")
        
        print("  âœ… detect_burst_words Passed!")
def generate_simulated_pipeline_data():
    """
    æ„é€ ç¬¦åˆ detect_burst_words é•¿åº¦è¦æ±‚çš„ä»¿çœŸæ•°æ®
    """
    print("ğŸ§ª Starting Pipeline Simulation...")
    
    # 1. å®ä¾‹åŒ–çœŸå®çš„é˜Ÿåˆ—
    hq = HistoryDataQueue(max_length=20) # ç¡®ä¿å®¹é‡å¤Ÿå¤§
    
    # ----------------------------------------
    # ç¬¬ä¸€é˜¶æ®µï¼šæ„é€  Baseline æ•°æ® (æ¨¡æ‹Ÿè¿‡å»çš„å¹³ç¨³æœŸ)
    # ----------------------------------------
    # æˆ‘ä»¬éœ€è¦æ„é€ è¶³å¤Ÿå¤šçš„å†å²æ•°æ®ï¼Œè®© baseline_window æœ‰ä¸œè¥¿å¯ç®—
    # å‡è®¾æˆ‘ä»¬æƒ³ç”¨ baseline_window_size = 5
    
    # åŸºç¡€è¯æ±‡ï¼šPythonä¸€ç›´å¾ˆç«ï¼ŒCrisisä»¥å‰å¾ˆå°‘
    base_words = ['Python', 'Data', 'Crisis', 'AI', 'Election']
    base_counts = [100,      80,     5,       60,   20]
    
    # å¾ªç¯æ·»åŠ  5 æ¬¡ï¼Œä½œä¸ºâ€œè¿‡å»çš„æ—¶é—´ç‰‡â€
    for i in range(5):
        # åŠ ä¸€ç‚¹éšæœºæ³¢åŠ¨ï¼Œæ˜¾å¾—çœŸå®
        counts = [c + np.random.randint(-5, 5) for c in base_counts]
        df_temp = pd.DataFrame({'word': base_words, 'count': counts})
        hq.add(df_temp)
        
    # ----------------------------------------
    # ç¬¬äºŒé˜¶æ®µï¼šæ„é€  Current æ•°æ® (æ¨¡æ‹Ÿæœ€è¿‘çš„çˆ†å‘æœŸ)
    # ----------------------------------------
    # æˆ‘ä»¬ç”¨ current_window_size = 2
    
    # T-1 (å€’æ•°ç¬¬äºŒæ¬¡): è¶‹åŠ¿å¼€å§‹å˜åŒ–
    df_prev = pd.DataFrame({
        'word': ['Python', 'Data', 'Crisis', 'AI', 'Election', 'SuddenMeme'],
        'count': [105,     82,     20,       65,   22,         50] 
    })
    hq.add(df_prev) # Crisis å¼€å§‹æ¶¨äº†, SuddenMeme å‡ºç°äº†
    
    # T-0 (æœ€æ–°ä¸€æ¬¡): å½»åº•çˆ†å‘
    # Crisis: 5 -> 20 -> 100 (Burst!)
    # SuddenMeme: 0 -> 50 -> 300 (New & Burst!)
    df_curr = pd.DataFrame({
        'word': ['Python', 'Data', 'Crisis', 'AI', 'Election', 'SuddenMeme', 'Unknown'],
        'count': [110,     85,     100,      70,   25,         300,          10]
    })
    hq.add(df_curr)
    
    # ----------------------------------------
    # ç¬¬ä¸‰é˜¶æ®µï¼šè°ƒç”¨ç®—æ³• (å…³é”®ç‚¹ï¼)
    # ----------------------------------------
    
    # ç›®å‰é˜Ÿåˆ—æ€»é•¿åº¦ = 5 (Base) + 2 (Current) = 7
    # ä½ çš„å‡½æ•°é»˜è®¤è¦æ±‚ 2+8=10ï¼Œå¦‚æœä¸æ”¹å‚æ•°ä¼šè¿”å›ç©ºã€‚
    # æ‰€ä»¥æˆ‘ä»¬æ‰‹åŠ¨ä¼ å…¥ window å‚æ•°ï¼Œé€‚é…ç°æœ‰çš„ 7 æ¡æ•°æ®ã€‚
    
    # baseline_window_size è®¾ä¸º 5ï¼Œcurrent è®¾ä¸º 2ï¼Œæ€»å…±éœ€è¦ 7ï¼Œåˆšå¥½æ»¡è¶³ã€‚
    burst_df = detect_burst_words(
        freq_queue=hq,                # ä¼ å…¥é˜Ÿåˆ—å®ä¾‹
        current_window_size=2,        # å–æœ€è¿‘2ä¸ª
        baseline_window_size=5,       # å–å†ä¹‹å‰çš„5ä¸ªåšåŸºçº¿
        min_freq_now=10,              # è¿‡æ»¤å¤ªå°çš„
        top_k=10
    )
    
    # è®¡ç®— Trend (å¸¸è§„é€»è¾‘)
    recent = hq.get_recent(2)
    df_t1 = recent[0][1]
    df_t0 = recent[1][1]
    trend_result = calculate_word_trends(df_t1, df_t0)
    
    # è¡¥ä¸ï¼šç¡®ä¿ change å­—æ®µå­˜åœ¨
    for key, df in trend_result.items():
        if not df.empty and 'diff' in df.columns:
            df['change'] = df['diff']

    return trend_result, burst_df
if __name__ == '__main__':
    unittest.main()