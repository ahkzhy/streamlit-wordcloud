import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt

from test_burst_trend import generate_simulated_pipeline_data
from utils import generate_simulated_sentiment_data
from wordcloud import WordCloud
import plotly.express as px
from datetime import datetime, timedelta
import re
from data_processing import analysisData
from updater import start_download_thread
import time
from datetime import datetime, timedelta
from streamlit_autorefresh import st_autorefresh # è®°å¾— pip install streamlit-autorefresh


# æ¯éš” 10000 æ¯«ç§’ï¼ˆ10ç§’ï¼‰å¼ºåˆ¶ Streamlit é‡æ–°è¿è¡Œä¸€éæ•´ä¸ªè„šæœ¬
st_autorefresh(interval=10000, limit=None, key="data_updater")

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼ˆé¿å…è­¦å‘Šï¼‰
plt.rcParams['font.family'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Trend Analysis Dashboard",
    page_icon="ğŸ“Š",
    layout="wide"
)

st.title("ğŸ“Š Real-time Trend Analysis Dashboard")
st.markdown("---")


def generate_wordcloud(df, word_col='word', weight_col='count', title="Word Cloud", max_words=100):
    """ç”Ÿæˆè¯äº‘å›¾"""
    if df is None or len(df) == 0:
        return None
    try:
        word_freq = dict(zip(df[word_col], df[weight_col]))
        wordcloud = WordCloud(
            width=900, height=450, background_color='white',
            colormap='viridis', max_words=max_words, relative_scaling=0.5
        ).generate_from_frequencies(word_freq)
        
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title(title, fontsize=16, pad=20)
        return fig
    except Exception as e:
        st.error(f"Error generating wordcloud: {e}")
        return None

def generate_combined_wordcloud(df, word_col='word', freq_col='count', tfidf_col='score', title="Combined Word Cloud", max_words=100):
    """ç”Ÿæˆç»“åˆfrequencyå’ŒTF-IDFçš„è¯äº‘"""
    if df is None or len(df) == 0:
        return None
    try:
        # æ ‡å‡†åŒ–frequencyå’ŒTF-IDFåˆ†æ•°
        freq_values = df[freq_col].values
        tfidf_values = df[tfidf_col].values
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆfrequency * TF-IDFï¼‰
        combined_scores = freq_values * tfidf_values
        
        # åˆ›å»ºç»¼åˆæƒé‡å­—å…¸
        word_weights = dict(zip(df[word_col], combined_scores))
        
        wordcloud = WordCloud(
            width=900, height=450, background_color='white',
            colormap='viridis', max_words=max_words, relative_scaling=0.5
        ).generate_from_frequencies(word_weights)
        
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title(title, fontsize=16, pad=20)
        return fig, word_weights
    except Exception as e:
        st.error(f"Error generating combined wordcloud: {e}")
        return None, None



def load_data_files():
    """åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
    data_files = {}
    file_mappings = {
    'word_freq': 'word_frequency.csv',
    'word_freq_title': 'word_frequency_title.csv',
    'tfidf': 'tfidf.csv',
    'tfidf_title': 'tfidf_title.csv',
    'articles': 'articles.csv'
}

    
    for key, filename in file_mappings.items():
        try:
            if key == 'articles':
                df = pd.read_csv(filename)
                # å°è¯•è§£ææ—¶é—´åˆ—
                if 'published_time' in df.columns:
                    df['published_time'] = pd.to_datetime(df['published_time'], errors='coerce')
            else:
                df = pd.read_csv(filename)
            
            # ç»Ÿä¸€åˆ—å
            if key in ['word_freq', 'word_freq_title'] and 'count' in df.columns:
                df = df.rename(columns={'count': 'frequency'})
            
            data_files[key] = df
            print(f"âœ… Loaded {filename}")
            
        except FileNotFoundError:
            print(f"âŒ {filename} not found, using sample data")
            # åˆ›å»ºç¤ºä¾‹æ•°æ®
            if key == 'articles':
                data_files[key] = pd.DataFrame({
                    'id': [1, 2, 3], 'country': ['US', 'UK', 'CA'],
                    'platform': ['news.com', 'blog.org', 'forum.net'],
                    'published_time': pd.to_datetime(['2025-11-13 10:00:00', '2025-11-13 11:00:00', '2025-11-13 12:00:00']),
                    'title': ['Sample 1', 'Sample 2', 'Sample 3'],
                    'content': ['Content 1', 'Content 2', 'Content 3'],
                    'url': ['http://example.com/1', 'http://example.com/2', 'http://example.com/3']
                })
            else:
                data_files[key] = pd.DataFrame({
                    'word': ['technology', 'innovation', 'data', 'analysis', 'research'],
                    'frequency' if key in ['word_freq', 'word_freq_title'] else 'score': [100, 80, 60, 40, 20]
                })
    
    return data_files


def update_data_cache():
    """
    ä»åç«¯ backend_engine è·å–æœ€æ–°æ•°æ®å¹¶é€‚é…å‰ç«¯æ ¼å¼
    """
    if 'backend_engine' not in st.session_state:
        st.error("Backend engine not initialized!")
        return

    backend = st.session_state.backend_engine
    with st.spinner("Loading data..."):
        # data_files = load_data_files()
        # if st.session_state.analysis_data is not None:
        #     backend.update_data()
        # ==========================================
        # 2. æå–è¯é¢‘æ•°æ® (é€‚é… HistoryDataQueue)
        # ==========================================
        
        # è·å–æœ€æ–°çš„å…ƒç»„ (timestamp, df)
        latest_content_tuple = backend.word_frequency_df.get_latest()
        latest_title_tuple = backend.word_frequency_title_df.get_latest()

        # è§£åŒ…å…ƒç»„ï¼Œå–å‡º DataFrameã€‚å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œç»™ä¸€ä¸ªç©ºçš„ DataFrame é˜²æ­¢æŠ¥é”™
        if latest_content_tuple:
            _, content_freq_df = latest_content_tuple # å¿½ç•¥æ—¶é—´æˆ³ï¼Œåªå– df
        else:
            content_freq_df = pd.DataFrame(columns=['word', 'frequency'])

        if latest_title_tuple:
            _, title_freq_df = latest_title_tuple
        else:
            title_freq_df = pd.DataFrame(columns=['word', 'frequency'])

        # ==========================================
        # 3. æå–å…¶ä»–æ•°æ® (ä¿æŒä¸å˜)
        # ==========================================
        content_tfidf_df = backend.tfidf_df
        title_tfidf_df = backend.tfidf_title_df
        articles_df = backend.article_df
        sentiment_trend_df = backend.get_sentiment_trend()
        # ==========================================
        # 4. æ„å»ºå‰ç«¯æ•°æ®å­—å…¸
        # ==========================================   
        analysis_data = {
            'word_data': {
                'content_freq': content_freq_df, 
                'content_tfidf': content_tfidf_df,
                'title_freq': title_freq_df,     
                'title_tfidf': title_tfidf_df
            },
            'platform_data': articles_df,
            'sentiment_data': { 
                'content_desc': backend.get_sentiment_content_top_10_desc(),
                'content_asc': backend.get_sentiment_content_top_10_asc(),
                'title_desc': backend.get_sentiment_title_top_10_desc(),
                'title_asc': backend.get_sentiment_title_top_10_asc(),
                'sentiment_trend': sentiment_trend_df
            },
            'trend_data': {
                'trends': backend.get_word_trends_analysis(),
                'bursts': backend.get_burst_words_analysis(),
                'history_trends': backend.get_history_trends_analysis()
            },
            'last_update': datetime.now(),
            'top_words': {},
            'top_platforms': None
        }
        
        # é¢„è®¡ç®—topè¯æ±‡
        for data_type, df in analysis_data['word_data'].items():
            if df is not None and len(df) > 0:
                weight_col = 'count' if 'freq' in data_type else 'score'
                analysis_data['top_words'][data_type] = df.nlargest(20, weight_col)
        
        # é¢„è®¡ç®—topå¹³å°
        if analysis_data['platform_data'] is not None:
            platform_counts = analysis_data['platform_data']['platform'].value_counts().reset_index()
            platform_counts.columns = ['platform', 'count']
            analysis_data['top_platforms'] = platform_counts
        
        st.session_state.analysis_data = analysis_data
        st.session_state.last_refresh = datetime.now().strftime("%H:%M:%S")

def get_articles_by_platform_and_words(platforms, top_words, articles_df, max_platforms=15):
    """æ ¹æ®å¹³å°å’Œå…³é”®è¯è·å–ç›¸å…³æ–‡ç« """
    result = {}
    
    # è·å–topå¹³å°
    top_platform_list = platforms.head(max_platforms)['platform'].tolist()
    
    # è·å–topè¯æ±‡
    keyword_list = top_words['word'].tolist()
    
    for platform in top_platform_list:
        platform_articles = articles_df[articles_df['platform'] == platform]
        
        relevant_articles = []
        for _, article in platform_articles.iterrows():
            title = str(article.get('title', ''))
            url = article.get('url', '')
            
            # å°†æ ‡é¢˜åˆ†å‰²æˆå•è¯åˆ—è¡¨ï¼ˆåªåŒ¹é…å®Œæ•´å•è¯ï¼‰
            title_words = re.findall(r'\b\w+\b', title.lower())
            
            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å®Œæ•´çš„topè¯æ±‡
            for keyword in keyword_list:
                keyword_lower = keyword.lower()
                if keyword_lower in title_words:
                    relevant_articles.append({
                        'title': title,
                        'url': url,
                        'matched_keyword': keyword
                    })
                    break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±åœæ­¢ï¼Œé¿å…é‡å¤
        
        if relevant_articles:
            result[platform] = relevant_articles
    
    return result

def main():
    # åˆå§‹åŒ–session state
    if 'backend_engine' not in st.session_state:
        engine = analysisData()
        start_download_thread(engine) 
        st.session_state.backend_engine = engine
    # åˆå§‹åŒ–session state
    if 'analysis_data' not in st.session_state:
        st.session_state.analysis_data = None
    if 'last_refresh' not in st.session_state:
        st.session_state.last_refresh = None
    if 'cache_expiry' not in st.session_state:
        st.session_state.cache_expiry = datetime.now()
    
    # åŠ è½½æˆ–æ›´æ–°æ•°æ®
    if st.session_state.analysis_data is None:
        update_data_cache()
        st.session_state.cache_expiry = datetime.now() + timedelta(seconds=10)
    
    data = st.session_state.analysis_data
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("Control Panel")
        
        data_type = st.radio("Data Type", ["Content Analysis", "Title Analysis"])
        weight_method = st.radio("Weight Method", ["Frequency", "TF-IDF", "Combined"])
        frequency_trend_type=st.radio("Frequency Trend",['common', 'new', 'lost','all'])
        debug_mode = st.checkbox("ğŸ›  Debug Mode (Use Fake Data)", value=False)
        # ç¼“å­˜çŠ¶æ€æ˜¾ç¤º
        if st.session_state.analysis_data:
            last_update = st.session_state.analysis_data['last_update']
            st.write(f"Last update: {last_update.strftime('%H:%M:%S')}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç¼“å­˜ï¼ˆæ¯3å°æ—¶ï¼‰
            if datetime.now() > st.session_state.cache_expiry:
                with st.spinner("Cache expired, refreshing data..."):
                    update_data_cache()
                    st.session_state.cache_expiry = datetime.now() + timedelta(seconds=10)
        
        if st.button("ğŸ”„ Refresh Data Now"):
            update_data_cache()
            st.session_state.cache_expiry = datetime.now() + timedelta(seconds=10)
            st.rerun()
    

    
    # ç¡®å®šå½“å‰æ•°æ®æº
    if data_type == "Content Analysis":
        current_freq_data = data['word_data']['content_freq']
        current_tfidf_data = data['word_data']['content_tfidf']
        current_data = current_freq_data if weight_method == "Frequency" else current_tfidf_data
        top_words_key = 'content_freq' if weight_method == "Frequency" else 'content_tfidf'
    else:
        current_freq_data = data['word_data']['title_freq']
        current_tfidf_data = data['word_data']['title_tfidf']
        current_data = current_freq_data if weight_method == "Frequency" else current_tfidf_data
        top_words_key = 'title_freq' if weight_method == "Frequency" else 'title_tfidf'
    
    weight_col = 'count' if weight_method == "Frequency" else 'score'
    
    # ä¸»å†…å®¹åŒº - å¢åŠ ç¬¬å››ä¸ªæ ‡ç­¾é¡µç”¨äºæ–‡ç« è·³è½¬
    tab1, tab2, tab3, tab4,tab5,tab6 = st.tabs([
        "â˜ï¸ Word Cloud", 
        "ğŸ“Š Platform Analysis", 
        "ğŸ“ˆ Data Details", 
        "ğŸ”— Article Links",
        "ğŸ­ Sentiment Analysis",
        "ğŸš€ Word Trends"])
    
    with tab1:
        if weight_method == "Combined":
            st.header(f"{data_type} - Combined Frequency & TF-IDF Word Cloud")
            if current_freq_data is not None and current_tfidf_data is not None:
                merged_data = pd.merge(current_freq_data, current_tfidf_data, on='word', suffixes=('_freq', '_tfidf'))
                combined_fig, combined_weights = generate_combined_wordcloud(
                    merged_data, 
                    freq_col='frequency', 
                    tfidf_col='score',
                    title=f"Combined Word Cloud ({data_type})"
                )
                if combined_fig:
                    st.pyplot(combined_fig)
                
                st.subheader("Top 10 Words (Combined Score)")
                if combined_weights:
                    top_words = sorted(combined_weights.items(), key=lambda x: x[1], reverse=True)[:10]
                    top_df = pd.DataFrame(top_words, columns=['word', 'combined_score'])
                    st.dataframe(top_df, use_container_width=True)
                    
                    # ä¿å­˜topè¯æ±‡ç”¨äºæ–‡ç« è·³è½¬
                    st.session_state.current_top_words = top_df
            else:
                st.warning("No data available for combined word cloud")
        
        else:
            st.header(f"{data_type} - {weight_method} Word Cloud")
            if current_data is not None and len(current_data) > 0:
                wordcloud_fig = generate_wordcloud(current_data, weight_col=weight_col)
                if wordcloud_fig:
                    st.pyplot(wordcloud_fig)
                
                st.subheader("Top 10 Words")
                top_data = current_data.nlargest(10, weight_col)
                st.dataframe(top_data, use_container_width=True)
                
                # ä¿å­˜topè¯æ±‡ç”¨äºæ–‡ç« è·³è½¬
                st.session_state.current_top_words = top_data
            else:
                st.warning("No data available")
    
    with tab2:
        st.header("Platform Distribution")
        if data['platform_data'] is not None and len(data['platform_data']) > 0:
            platform_counts = data['top_platforms']
            
            max_platforms = st.slider("Number of platforms to display", 
                                     min_value=5, 
                                     max_value=min(30, len(platform_counts)), 
                                     value=15)
            
            top_platforms = platform_counts.head(max_platforms)
            
            col1, col2 = st.columns(2)
            with col1:
                fig_pie = px.pie(
                    top_platforms, 
                    names='platform', 
                    values='count', 
                    title=f"Top {max_platforms} Platforms Distribution"
                )
                st.plotly_chart(fig_pie, use_container_width=True)
            
            with col2:
                fig_bar = px.bar(
                    top_platforms, 
                    x='count', 
                    y='platform', 
                    orientation='h', 
                    title=f"Top {max_platforms} Platforms",
                    color='count',
                    color_continuous_scale='viridis'
                )
                fig_bar.update_layout(
                    yaxis={'categoryorder': 'total ascending'},
                    showlegend=False
                )
                st.plotly_chart(fig_bar, use_container_width=True)
            
            # ä¿å­˜topå¹³å°ç”¨äºæ–‡ç« è·³è½¬
            st.session_state.current_top_platforms = top_platforms
            
            st.subheader("Platform Statistics")
            col_stat1, col_stat2, col_stat3 = st.columns(3)
            with col_stat1:
                st.metric("Total Platforms", len(platform_counts))
            with col_stat2:
                coverage = (top_platforms['count'].sum() / platform_counts['count'].sum() * 100) if platform_counts['count'].sum() > 0 else 0
                st.metric(f"Top {max_platforms} Platforms Coverage", f"{coverage:.1f}%")
            with col_stat3:
                st.metric("Most Frequent Platform", 
                         top_platforms.iloc[0]['platform'] if len(top_platforms) > 0 else "N/A")
        else:
            st.info("No article data available")
    
    with tab3:
        st.header("Data Statistics")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Last Update", st.session_state.last_refresh)
        with col2:
            if weight_method == "Combined":
                data_to_count = current_freq_data if current_freq_data is not None else current_data
            else:
                data_to_count = current_data
            st.metric("Total Words", len(data_to_count) if data_to_count is not None else 0)
        with col3:
            st.metric("Data Type", data_type)
        with col4:
            st.metric("Weight Method", weight_method)
    
    with tab4:
        st.header("ğŸ”— Relevant Articles by Platform")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ•°æ®
        if ('current_top_words' not in st.session_state or 
            'current_top_platforms' not in st.session_state or 
            data['platform_data'] is None):
            st.info("Please view Word Cloud and Platform Analysis first to load the necessary data.")
        else:
            top_words = st.session_state.current_top_words
            top_platforms = st.session_state.current_top_platforms
            
            st.subheader("Search Criteria")
            col1, col2 = st.columns(2)
            with col1:
                st.write("**Top Words:**", ", ".join(top_words['word'].head(10).tolist()))
            with col2:
                st.write("**Top Platforms:**", ", ".join(top_platforms['platform'].head(10).tolist()))
            
            if st.button("ğŸ” Find Relevant Articles"):
                with st.spinner("Searching for relevant articles..."):
                    relevant_articles = get_articles_by_platform_and_words(
                        top_platforms, 
                        top_words,
                        data['platform_data']
                    )
                    
                    if relevant_articles:
                        st.success(f"Found relevant articles from {len(relevant_articles)} platforms")
                        
                        for platform, articles in relevant_articles.items():
                            with st.expander(f"ğŸ“° {platform} ({len(articles)} articles)"):
                                for i, article in enumerate(articles, 1):
                                    st.write(f"{i}. **{article['title']}**")
                                    st.write(f"   ğŸ”— [Open Article]({article['url']})")
                                    st.write(f"   ğŸ¯ Matched keyword: `{article['matched_keyword']}`")
                                    st.write("---")
                    else:
                        st.warning("No relevant articles found matching the criteria.")

    with tab5:
        st.header("Sentiment Analysis")
        
        # ä» session_state è·å–åˆšæ‰å­˜è¿›å»çš„æƒ…æ„Ÿæ•°æ®
        sent_data = st.session_state.analysis_data.get('sentiment_data')
        
        if sent_data:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ğŸ˜¡ Most Negative Content")
                st.dataframe(sent_data['content_asc'], use_container_width=True)
                
                st.subheader("ğŸ˜Š Most Positive Content")
                st.dataframe(sent_data['content_desc'], use_container_width=True)
                
            with col2:
                st.subheader("ğŸ˜¡ Most Negative Titles")
                st.dataframe(sent_data['title_asc'], use_container_width=True)
                
                st.subheader("ğŸ˜Š Most Positive Titles")
                st.dataframe(sent_data['title_desc'], use_container_width=True)
        else:
            st.info("Sentiment data is not available.")
        
        st.markdown("---") # åˆ†å‰²çº¿
        # === æ–°å¢ï¼šæƒ…æ„Ÿè¶‹åŠ¿éƒ¨åˆ† ===
        st.subheader("â¤ï¸ Sentiment Trend Over Time")

        # è·å–æ•°æ®
        sent_trend = data.get('trend_data', {}).get('sentiment_trend', pd.DataFrame())
        if debug_mode:
            st.warning("ğŸ§ª Using Simulated Data generated by Real Algorithms")
            # è°ƒç”¨æµ‹è¯•ç”¨æ•°æ®
            sent_trend =generate_simulated_sentiment_data()
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ® (åŒ…æ‹¬çœŸå®æ•°æ® æˆ– åé¢ä»¿çœŸç”Ÿæˆçš„å‡æ•°æ®)
        if not sent_trend.empty:
            # Plotly éœ€è¦é•¿æ ¼å¼ (Long Format) æ¥ç”»å¤šæ¡çº¿ï¼Œæˆ–è€…æˆ‘ä»¬ç›´æ¥æ‰‹åŠ¨æ·»åŠ  trace
            # è¿™é‡Œæˆ‘ä»¬ç®€å•ç”»ä¸ªåŒçº¿å›¾
            
            # ä½¿ç”¨ Plotly Express
            # è¿™é‡Œçš„ x è½´æ˜¯æ—¶é—´ï¼Œy è½´æ˜¯åˆ†æ•°
            fig_sent = px.line(
                sent_trend, 
                x='timestamp', 
                y=['avg_content', 'avg_title'], # åŒæ—¶ç”»ä¸¤æ¡çº¿
                markers=True,
                title="Average Sentiment Score History",
                labels={'value': 'Sentiment Score', 'timestamp': 'Time', 'variable': 'Source'},
                template="plotly_white"
            )
            
            # ä¼˜åŒ–ä¸€ä¸‹ Y è½´èŒƒå›´ (é€šå¸¸æƒ…æ„Ÿåˆ†æ˜¯ -1 åˆ° 1ï¼Œæˆ–è€… 0 åˆ° 1ï¼Œæ ¹æ®ä½ çš„æ•°æ®è°ƒæ•´)
            fig_sent.update_yaxes(range=[-1.1, 1.1]) # å‡è®¾åˆ†æ•°æ˜¯ -1(è´Ÿé¢) åˆ° 1(æ­£é¢)
            
            # æ·»åŠ å‚è€ƒçº¿ (0åˆ†çº¿)
            fig_sent.add_hline(y=0, line_dash="dash", line_color="gray", annotation_text="Neutral")
            
            st.plotly_chart(fig_sent, use_container_width=True)
        else:
            st.info("Accumulating sentiment history... (Wait for next update)")

    with tab6:
        st.header("ğŸš€ Trend & Burst Analysis")
        history_trend_dict = data['trend_data']['history_trends']
        trend_dict = data['trend_data']['trends']
        burst_df = data['trend_data']['bursts']
        if debug_mode:
            st.warning("ğŸ§ª Using Simulated Data generated by Real Algorithms")
            # è°ƒç”¨æµ‹è¯•ç”¨æ•°æ®
            trend_dict, burst_df = generate_simulated_pipeline_data()
        col_t1, col_t2 = st.columns(2)
        
        with col_t1:
            st.subheader("ğŸ“ˆ Trending Keywords")
    
            # ä½¿ç”¨ Tab åˆ†é¡µå±•ç¤ºä¸¤ç§ä¸åŒçš„è§†è§’
            tab_short, tab_long = st.tabs(["âš¡ Vs Last Update", "ğŸ“… Vs History Avg"])
            with tab_short:
                st.caption("Comparing current frequency with the immediate previous record.")
                if trend_dict:
                    if frequency_trend_type:
                        trend_df=trend_dict[frequency_trend_type]
                        word = trend_df.head(10)
                        
                    else:
                        trend_df=trend_dict["all"]
                        word=trend_df.head(10)
                    # st.dataframe(word, use_container_width=True)
                    if word is not None and not word.empty:
                        cols = st.columns(2) # 2åˆ—å¸ƒå±€
                        for idx, (index, row) in enumerate(word.iterrows()):
                            c = cols[idx % 2]
                            
                            # å°è¯•è·å– count å’Œ changeï¼Œå¦‚æœæ²¡æœ‰åˆ™ç»™é»˜è®¤å€¼
                            val = row.get('count_new', 0)
                            diff = row.get('freq_change', 0) 
                            
                            c.metric(
                                label=f"{row['word']}", 
                                value=int(val), 
                                delta=f"{int(diff)}" if diff != 0 else None
                            )
                    else:
                        st.info("No trend data available.")
                else:
                    st.info("Not enough history data to calculate trends yet. (Need at least 2 updates)")
            with tab_long:
                st.caption("Comparing current frequency with the average of all history.")
                if history_trend_dict:
                    if frequency_trend_type:
                        trend_df=history_trend_dict[frequency_trend_type]
                        word = trend_df.head(10)
                        
                    else:
                        trend_df=history_trend_dict["all"]
                        word=trend_df.head(10)
                    # st.dataframe(word, use_container_width=True)
                    if word is not None and not word.empty:
                        cols = st.columns(2) # 2åˆ—å¸ƒå±€
                        for idx, (index, row) in enumerate(word.iterrows()):
                            c = cols[idx % 2]
                            
                            # å°è¯•è·å– count å’Œ changeï¼Œå¦‚æœæ²¡æœ‰åˆ™ç»™é»˜è®¤å€¼
                            val = row.get('count_new', 0)
                            diff = row.get('freq_change', 0) 
                            
                            c.metric(
                                label=f"{row['word']}", 
                                value=int(val), 
                                delta=f"{int(diff)}" if diff != 0 else None
                            )
                    else:
                        st.info("No trend data available.")
                else:
                    st.info("Not enough history data to calculate trends yet. (Need at least 3 updates)")                
        with col_t2:
            st.subheader("ğŸ’¥ Burst Words (Sudden Spikes)")
            if not burst_df.empty:
                if 'burst_score' in burst_df.columns:
                    bursts = burst_df.head(10)
                    st.dataframe(bursts, use_container_width=True)
                    
                    if not bursts.empty:
                        # fig = px.bar(bursts, x='word', y='burst_score', title="Top Burst Scores", color='burst_score')
                        # st.plotly_chart(fig, use_container_width=True)
                        fig = px.scatter(
                            bursts, 
                            x='freq_now', 
                            y='burst_score', 
                            size='fold_change',  # æ°”æ³¡å¤§å°ä»£è¡¨ç¿»äº†å¤šå°‘å€
                            color='burst_score', # é¢œè‰²ä»£è¡¨çˆ†å‘å¾—åˆ†
                            hover_name='word',   # é¼ æ ‡æ‚¬åœæ˜¾ç¤ºå•è¯
                            size_max=40,         # æ°”æ³¡æœ€å¤§å°ºå¯¸
                            title="ğŸ’¥ Burst Intensity vs. Volume",
                            labels={'freq_now': 'Current Volume', 'burst_score': 'Burst Intensity'},
                            template="plotly_white"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                else:
                    st.dataframe(burst_df)
            else:
                st.info("No burst words detected or insufficient history.")

if __name__ == "__main__":
    main()
